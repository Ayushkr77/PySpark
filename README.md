# PySpark Full Course üöÄ

This repository contains a step-by-step tutorial series on **Apache Spark using PySpark in Databricks**. It covers everything from Spark basics to advanced topics like Joins, Window Functions, and Spark SQL.  

---

## üìå Topics Covered

### 1. Data Reading & Schema
- Data Reading  
- Data Reading JSON  
- Schema Definition  
- DDL Schema  

### 2. Data Selection & Filtering
- SELECT  
- ALIAS  
- FILTER (Scenarios 1‚Äì3)  
- withColumnRenamed  
- withColumn  
- Type Casting  

### 3. Data Operations
- sort  
- LIMIT  
- DROP  
- Drop_Duplicates  
- UNION and UNION BY NAME  
  - Preparing DataFrames  
  - Union  
  - Union by Name  

### 4. String & Date Functions
- String Functions: `initcap()`, `lower()`, `upper()`  
- Date Functions: `current_date`, `date_add()`, `date_sub()`, `datediff`  

### 5. Handling Nulls
- Dropping Nulls  
- Filling Nulls  

### 6. Array Operations
- SPLIT and Indexing  
  - SPLIT  
  - Indexing  
  - Explode  

### 7. Aggregations
- GroupBy (Scenarios)  
- collect_list  
  - Creating Sample DataFrame  
  - Group & Collect Books per User  
  - Select Specific Columns  
- Pivot  

### 8. Conditional & Joins
- When‚ÄìOtherwise  
- Joins:  
  - Inner Join  
  - Left Join  
  - Right Join  
  - Anti Join  

### 9. Advanced Functions
- Window Functions  
  - Row_Number()  
  - Rank vs Dense Rank  
- User Defined Functions (UDF)  
  - Step 1  
  - Step 2  

### 10. Data Writing
- Writing to CSV  
  - Overwrite  
  - Error  
  - Ignore  
- Writing to Parquet  
- Writing to Table  

### 11. Spark SQL
- createTempView  
- Running SQL Queries in Spark  

---

## üõ†Ô∏è Requirements
- Databricks Community Edition (or Databricks Workspace)  
- Python 3.8+  
- PySpark  

---

## üöÄ How to Run
1. Import the notebook into **Databricks**.  
2. Attach a cluster.  
3. Run all the cells step by step to practice PySpark.  

---

## üéØ Goal
The goal of this repo is to **learn PySpark hands-on in Databricks** with real examples and structured notes. By the end, you‚Äôll have a strong understanding of how to process big data using Apache Spark.
